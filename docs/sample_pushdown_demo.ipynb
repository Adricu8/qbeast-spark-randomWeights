{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Qbeast Datasource Format\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/Qbeast-spark.png\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "## Content\n",
    "- Introduction\n",
    "- Table Indexing\n",
    "- Sample Pushdown\n",
    "- Table Tolerance\n",
    "- Analyze and Optimize\n",
    "\n",
    "## Predicate PushDown\n",
    "**Predicate pushdown** is of great importance when it comes to **optimizing the logical plan** of a query. Among its benefits, one can find reduced usage of computation resources and less I/O from the secondary storage.\n",
    "\n",
    "Having a **predicate** in a query generally means the subsequent operators will work with fewer data. Without affecting the query output, further down the plan are the filters greater the benefits.\n",
    "\n",
    "```sql\n",
    "SELECT A.name\n",
    "FROM A JOIN B ON A.id = B.id\n",
    "WHERE CONDITION_ON_A AND CONDITION_ON_B\n",
    "\n",
    "```\n",
    "\n",
    "Take the query above as an example, a potential physical plan, without predicate pushdown can be as following:\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/g1.png\" width=\"400\" height=\"500\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "It starts by reading all the data from both tables **A** and **B**, performs the join on the id column, **A.id = B.id**, proceed to apply the **predicates** specified by the **WHERE** clause, **cond_A**, and **cond_B**, and finally project the target column, **name**.\n",
    "\n",
    "With the optimization of **predicate pushdown**, both conditions are found at the source and used as filters to select satisfying records, **reducing disk I/O** in this way. The **join** operator now also gets to operate with fewer data.\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/g2.png\" width=\"400\" height=\"500\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "As **query complexity** increases, the usage of **optimization rules** ensures moving the predicate is safe from altering the final output. Such rules are present in all major SQL query engines, with **Spark SQL** being no exception.\n",
    "\n",
    "\n",
    "## Sampling\n",
    "\n",
    "**Sample** operators are yet another way to achieve the benefits of working with a reduced dataset. Unlike using filters where record selection is done **deterministically**, a (uniform) sample operator constructs a **representative subset** of the original data **randomly and uniformly**. The resulting subset is expected to have a **distribution** that resembles the source, and users generally only need to provide the **fraction** of the source data they desire to work with.\n",
    "\n",
    "```python\n",
    "df = spark.read.load(source_path)\n",
    "df.sample(fraction=0.5)\n",
    "```\n",
    "\n",
    "Its usage reduces **compute cost and latency** as before, except the result **accuracy** is inevitably compromised in relation to the subset size.\n",
    "\n",
    "Unlike filters, a sampler operator in Spark SQL can only do its job once all the data is retrieved from the source, for which a complete disk I/O is still required. Apart from that, there's no clear model to understand the **cost and latency vs. accuracy** trade-off when choosing the **fraction** to use, the reason for which there is a general avoidance for using samplers.\n",
    "\n",
    "\n",
    "## Qbeast Format\n",
    "\n",
    "To address the above-mentioned issues, we introduce **qbeast datasource format** for Spark, a custom DataSource designed to enable **multidimensional indexing** for datasets together a set of transformation rules we achieve not only to convert the Sample operator into filters so random and uniform record selection can take place at the source, but on top of that, we've also created our own operator, **Table Tolerance**, which given the maximum query **tolerance** it can determine by itself the most cost-effective **fraction** to use, for which the user is no longer left wondering whether the sample they chose to use is accurate for their objectives."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Download Pyspark version 3.1.1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!conda install pyspark=3.1.1 -y"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing dependencies and initialize a Spark session"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "DATA_ROOT = \"/tmp/qbeast-test/data\"\n",
    "parquet_table_path = \"s3a://qbeast-public-datasets/store_sales\"\n",
    "qbeast_table_path = os.path.join(DATA_ROOT, \"qbeast/qtable\")\n",
    "\n",
    "\n",
    "hadoop_deps = ','.join(map(lambda a: 'org.apache.hadoop:hadoop-' + a + ':3.2.0', ['common','client','aws']))\n",
    "deps = \"io.delta:delta-core_2.12:0.8.0,com.amazonaws:aws-java-sdk:1.12.20,\" + hadoop_deps\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.jars\", \"../target/scala-2.12/qbeast-spark-assembly-0.1.0.jar\")\n",
    "         .config(\"spark.sql.extensions\", \"io.qbeast.spark.sql.QbeastSparkSessionExtension\")\n",
    "         .config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "         .config(\"spark.jars.packages\", deps)\n",
    "         .config(\"spark.driver.extraJavaOptions\", \"-Dqbeast.index.size=300000\")\n",
    "         .getOrCreate())\n",
    "spark.sparkContext.setLogLevel('WARN')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table Indexing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset used here is the **store_sales** table from **TCP-DS**. The format is parquet and its schema is shown below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "parquet_df = spark.read.format(\"parquet\").load(parquet_table_path)\n",
    "\n",
    "print(\"Number of rows with na:\", parquet_df.count())\n",
    "\n",
    "# Display the schema\n",
    "parquet_df.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of rows with na: 2879789\n",
      "root\n",
      " |-- ss_sold_time_sk: integer (nullable = true)\n",
      " |-- ss_item_sk: integer (nullable = true)\n",
      " |-- ss_customer_sk: integer (nullable = true)\n",
      " |-- ss_cdemo_sk: integer (nullable = true)\n",
      " |-- ss_hdemo_sk: integer (nullable = true)\n",
      " |-- ss_addr_sk: integer (nullable = true)\n",
      " |-- ss_store_sk: integer (nullable = true)\n",
      " |-- ss_promo_sk: integer (nullable = true)\n",
      " |-- ss_ticket_number: long (nullable = true)\n",
      " |-- ss_quantity: integer (nullable = true)\n",
      " |-- ss_wholesale_cost: decimal(7,2) (nullable = true)\n",
      " |-- ss_list_price: decimal(7,2) (nullable = true)\n",
      " |-- ss_sales_price: decimal(7,2) (nullable = true)\n",
      " |-- ss_ext_discount_amt: decimal(7,2) (nullable = true)\n",
      " |-- ss_ext_sales_price: decimal(7,2) (nullable = true)\n",
      " |-- ss_ext_wholesale_cost: decimal(7,2) (nullable = true)\n",
      " |-- ss_ext_list_price: decimal(7,2) (nullable = true)\n",
      " |-- ss_ext_tax: decimal(7,2) (nullable = true)\n",
      " |-- ss_coupon_amt: decimal(7,2) (nullable = true)\n",
      " |-- ss_net_paid: decimal(7,2) (nullable = true)\n",
      " |-- ss_net_paid_inc_tax: decimal(7,2) (nullable = true)\n",
      " |-- ss_net_profit: decimal(7,2) (nullable = true)\n",
      " |-- ss_sold_date_sk: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The table contains 23 columns in total, and the reason why only work with the first 5 is to have a cleaner query plan for later examination."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "processed_parquet_df = (\n",
    "    parquet_df\n",
    "    .select(\n",
    "        \"ss_sold_time_sk\",\n",
    "        \"ss_item_sk\",\n",
    "        \"ss_customer_sk\",\n",
    "        \"ss_cdemo_sk\",\n",
    "        \"ss_hdemo_sk\") # Selecting only the first 5 columns\n",
    "    .na.drop()         # dropping rows with null values\n",
    ")\n",
    "\n",
    "print(f\"Number of rows in the resulting dataframe: {processed_parquet_df.count()}\")\n",
    "processed_parquet_df.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of rows in the resulting dataframe: 2637520\n",
      "root\n",
      " |-- ss_sold_time_sk: integer (nullable = true)\n",
      " |-- ss_item_sk: integer (nullable = true)\n",
      " |-- ss_customer_sk: integer (nullable = true)\n",
      " |-- ss_cdemo_sk: integer (nullable = true)\n",
      " |-- ss_hdemo_sk: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the dataset set in place, we can write the table into a **qbeast datasource**, and indexing it using columns **ss_cdemo_sk** and **ss_hdemo_sk**. The choice of columns is trivial, at the moment any numerical column would do the trick. Generally one should choose the columns that they query most frequently on."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(processed_parquet_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"qbeast\")                                     # Saving the dataframe in a qbeast datasource\n",
    "    .option(\"columnsToIndex\", \"ss_cdemo_sk,ss_hdemo_sk\")  # Indexing the table\n",
    "    .save(qbeast_table_path)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling PushDown\n",
    "\n",
    "## Qbeast sample vs Spark vanilla sample\n",
    "\n",
    "To demonstrate the transformation of the **Sample** operator into **Filters** and the subsequent application of Predicate PushDown, we will examine the query plan of a sample operation on a qbeast table and compare it with its application on a regular parquet table, namely **processed_parquet_df** from above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# write the processed parquet data to a new folder \n",
    "# and re-read it so the query plan is simpler to examine.\n",
    "processed_parquet_dir = os.path.join(DATA_ROOT, \"parquet/test_data\")\n",
    "\n",
    "processed_parquet_df.write.mode(\"overwrite\").format(\"parquet\").save(processed_parquet_dir)\n",
    "\n",
    "processed_parquet_df = spark.read.format(\"parquet\").load(processed_parquet_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "qbeast_df = spark.read.format(\"qbeast\").load(qbeast_table_path)\n",
    "\n",
    "assert qbeast_df.count() == processed_parquet_df.count(), \"Both tables should have the same number of rows\"\n",
    "# 2.637.520"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(\"Query Plan for Sampling on a parquet file\\n\")\n",
    "\n",
    "processed_parquet_df.sample(fraction=0.1).explain(True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query Plan for Sampling on a parquet file\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Sample 0.0, 0.1, false, -4574351903056053551\n",
      "+- Relation[ss_sold_time_sk#1425,ss_item_sk#1426,ss_customer_sk#1427,ss_cdemo_sk#1428,ss_hdemo_sk#1429] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ss_sold_time_sk: int, ss_item_sk: int, ss_customer_sk: int, ss_cdemo_sk: int, ss_hdemo_sk: int\n",
      "Sample 0.0, 0.1, false, -4574351903056053551\n",
      "+- Relation[ss_sold_time_sk#1425,ss_item_sk#1426,ss_customer_sk#1427,ss_cdemo_sk#1428,ss_hdemo_sk#1429] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sample 0.0, 0.1, false, -4574351903056053551\n",
      "+- Relation[ss_sold_time_sk#1425,ss_item_sk#1426,ss_customer_sk#1427,ss_cdemo_sk#1428,ss_hdemo_sk#1429] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Sample 0.0, 0.1, false, -4574351903056053551\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [ss_sold_time_sk#1425,ss_item_sk#1426,ss_customer_sk#1427,ss_cdemo_sk#1428,ss_hdemo_sk#1429] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/tmp/qbeast-test/data/parquet/test_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ss_sold_time_sk:int,ss_item_sk:int,ss_customer_sk:int,ss_cdemo_sk:int,ss_hdemo_sk:int>\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice in the query plan for the parquet table that sample is the last operator from the query and it has remained that way for all stages of the query engine execution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(\"\\nQuery Plan for Sampling on a qbeast\\n\")\n",
    "\n",
    "qbeast_df.sample(fraction=0.1).explain(True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Query Plan for Sampling on a qbeast\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Sample 0.0, 0.1, false, 8938481429324116910\n",
      "+- Relation[ss_sold_time_sk#1511,ss_item_sk#1512,ss_customer_sk#1513,ss_cdemo_sk#1514,ss_hdemo_sk#1515] QbeastBaseRelation(parquet,WrappedArray(ss_cdemo_sk, ss_hdemo_sk))\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ss_sold_time_sk: int, ss_item_sk: int, ss_customer_sk: int, ss_cdemo_sk: int, ss_hdemo_sk: int\n",
      "Sample 0.0, 0.1, false, 8938481429324116910\n",
      "+- Relation[ss_sold_time_sk#1511,ss_item_sk#1512,ss_customer_sk#1513,ss_cdemo_sk#1514,ss_hdemo_sk#1515] QbeastBaseRelation(parquet,WrappedArray(ss_cdemo_sk, ss_hdemo_sk))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter ((qbeast_hash(ss_cdemo_sk#1514, ss_hdemo_sk#1515, 42) < -1717986918) AND (qbeast_hash(ss_cdemo_sk#1514, ss_hdemo_sk#1515, 42) >= -2147483648))\n",
      "+- Relation[ss_sold_time_sk#1511,ss_item_sk#1512,ss_customer_sk#1513,ss_cdemo_sk#1514,ss_hdemo_sk#1515] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter ((qbeast_hash(ss_cdemo_sk#1514, ss_hdemo_sk#1515, 42) < -1717986918) AND (qbeast_hash(ss_cdemo_sk#1514, ss_hdemo_sk#1515, 42) >= -2147483648))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [ss_sold_time_sk#1511,ss_item_sk#1512,ss_customer_sk#1513,ss_cdemo_sk#1514,ss_hdemo_sk#1515] Batched: true, DataFilters: [(qbeast_hash(ss_cdemo_sk#1514, ss_hdemo_sk#1515, 42) < -1717986918), (qbeast_hash(ss_cdemo_sk#15..., Format: Parquet, Location: OTreeIndex[file:/tmp/qbeast-test/data/qbeast/qtable], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ss_sold_time_sk:int,ss_item_sk:int,ss_customer_sk:int,ss_cdemo_sk:int,ss_hdemo_sk:int>\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the other hand, the sample operator is no longer present for the table with qbeast source, the optimized logical plan has a Filter that uses **qbeast_hash** to eliminate unnecessary data instead.\n",
    "\n",
    "Notice that the query plans for both dataframes are the same at the begining and they only started to differ after the application of the optimization rules, which in this case converted the **Sample** operator into **Filters** and applied **Predicate PushDown** rules from Spark query engine.\n",
    "\n",
    "These filters are pushed down to the level of the data source in the physical plan and are used by Spark as it scans the data from the source relation. The filters applied at the source are shown in the **DataFilters** filed from **FileScan parquet**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# processed_parquet_df.sample(0.1).collect()\n",
    "# qbeast_df.sample(fraction=0.1).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute the queries from the previous cell, and check the query plans from **Spark UI**.\n",
    "\n",
    "The fact that less files are accessed can be seen by comparing the total number of files in the folder and the number of files read from **Query Details**. Also, the **number of output rows** from **Scan parquet** can also indicate whether we are reading all the files.\n",
    "\n",
    "|Data Source              |Total number of files    |Number of files read     |Number of rows read    | Number of output rows\n",
    "|-------------------------|:-----------------------:|:-----------------------:|:-----------------------:|:-----------------------:|\n",
    "|parquet                  |16                        |16                        |2,637,520                | 264,192\n",
    "|qbeast                   |12                     |1                       |302,007               |262,013\n",
    "\n",
    "Under the hood, a qbeast table is divided into different partitions according to their states, and each partition is stored in a different parquet file. The filtering at the source is used for partition selection, and the second filtering is the one actually applied to the individual rows."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}